{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    calc_grad=True \n",
    "\n",
    "    \"\"\"calc_grad: bool, signaling whether to carry the gradients while artihmetic operations take place\"\"\"\n",
    "\n",
    "    def __init__(self,array,\n",
    "                grad=None,\n",
    "                requires_grad=False):\n",
    "        \"\"\"\n",
    "        array: numpy array\n",
    "        grad: dic={id(object): numpy.array}\n",
    "        requires_grad: bool, signaling whether to calculate or not the derivative relative to this tensor\n",
    "        \n",
    "        \"\"\"\n",
    "        self.array=array\n",
    "        self.requires_grad=requires_grad\n",
    "        \n",
    "        if requires_grad:\n",
    "            name=id(self) \n",
    "            self.grad={name: self.make_grad()}\n",
    "        else:\n",
    "            self.grad={'none':0}\n",
    "        if grad is not None:\n",
    "            self.grad=grad\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.array.shape\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.array.ndim\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.array.T\n",
    "\n",
    "    def make_grad(self,):\n",
    "        shape=self.array.shape\n",
    "        Kron=1\n",
    "        for d in shape:\n",
    "            ID=np.identity(d)\n",
    "            Kron=np.tensordot(Kron,ID,axes=0)\n",
    "        new_shape=[i for i in range(0,2*len(shape),2)]\n",
    "        new_shape+=[i for i in range(1,2*len(shape),2)]\n",
    "        Kron=Kron.transpose(new_shape)\n",
    "\n",
    "        return Kron\n",
    "\n",
    "    def check_grads(self,x):\n",
    "\n",
    "        for w in self.grad:\n",
    "            if w not in x.grad:\n",
    "                x.grad[w]=0\n",
    "        for w in x.grad:\n",
    "            if w not in self.grad:\n",
    "                self.grad[w]=0\n",
    "\n",
    "    def __add__(self,x):\n",
    "        \n",
    "        if isinstance(x,Tensor):\n",
    "            result=self.array+x.array\n",
    "            if self.calc_grad:\n",
    "                self.check_grads(x)\n",
    "                grad={}\n",
    "                for w in self.grad:\n",
    "                    grad[w]=self.grad[w]+x.grad[w]\n",
    "                return Tensor(result,grad=grad)\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "\n",
    "        if isinstance(x,int) or isinstance(x,float):\n",
    "            result=self.array+x\n",
    "            if self.calc_grad:\n",
    "                return Tensor(result,grad=self.grad.copy())\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "    \n",
    "    \n",
    "    def __radd__(self,x):\n",
    "\n",
    "        if isinstance(x,int) or isinstance(x,float):\n",
    "            result=self.array+x\n",
    "            if self.calc_grad:\n",
    "                return Tensor(result,grad=self.grad.copy())\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "\n",
    "    def __sub__(self,x):\n",
    "        \n",
    "        if isinstance(x,Tensor):\n",
    "            result=self.array-x.array\n",
    "            if self.calc_grad:\n",
    "                self.check_grads(x)\n",
    "                grad={}\n",
    "                for w in self.grad:\n",
    "                    grad[w]=self.grad[w]-x.grad[w]\n",
    "\n",
    "                return Tensor(result,grad=grad)\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "\n",
    "        if isinstance(x,int) or isinstance(x,float):\n",
    "            result=self.array-x\n",
    "            if self.calc_grad:\n",
    "                return Tensor(result,grad=self.grad.copy())\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "    \n",
    "    def __rsub__(self,x):\n",
    "        \n",
    "        if isinstance(x,int) or isinstance(x,float):\n",
    "            result=x-self.array\n",
    "            if self.calc_grad:\n",
    "                grad={}\n",
    "                for w in self.grad:\n",
    "                    grad[w]=-self.grad[w]\n",
    "                return Tensor(result,grad=grad)\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "\n",
    "    def __mul__(self,x):\n",
    "\n",
    "        if isinstance(x,int) or isinstance(x,float):\n",
    "            result=x*self.array\n",
    "            if self.calc_grad:\n",
    "                grad={}\n",
    "                for w in self.grad:\n",
    "                    grad[w]=x*self.grad[w]\n",
    "                return Tensor(result,grad=grad)\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "\n",
    "        if isinstance(x,Tensor):\n",
    "            result=np.tensordot(self.array,x.array,axes=([-1],[0]))\n",
    "            if self.calc_grad:\n",
    "                self.check_grads(x)\n",
    "                grad={}\n",
    "                for w in self.grad:\n",
    "                    if x.grad[w] is 0:\n",
    "                        grad1=0\n",
    "                    else:\n",
    "                        grad1=np.tensordot(self.array,x.grad[w],axes=([-1],[0]))\n",
    "                        \n",
    "                    if self.grad[w] is 0:\n",
    "                        grad2=0\n",
    "                    else:\n",
    "                        i=len(self.array.shape)\n",
    "                        grad2=np.tensordot(self.grad[w],x.array,axes=([i-1],[0]))\n",
    "                        n1=self.grad[w].ndim\n",
    "                        n2=self.array.ndim\n",
    "                        n3=x.array.ndim\n",
    "                        r1=[j for j in range(n2-1)]+[j for j in range(n1-1,n1+n3-2)]\n",
    "                        r2=[j for j in range(n2-1,n1-1)]\n",
    "                        grad2=grad2.transpose(r1+r2)\n",
    "                    \n",
    "                    grad[w]=grad1+grad2\n",
    "\n",
    "                return Tensor(result,grad=grad)\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "    \n",
    "    def __rmul__(self, x):\n",
    "        if isinstance(x,int) or isinstance(x,float):\n",
    "            result=x*self.array\n",
    "            if self.calc_grad:\n",
    "                grad={}\n",
    "                for w in self.grad:\n",
    "                    grad[w]=x*self.grad[w]\n",
    "                return Tensor(result,grad=grad)\n",
    "            else:\n",
    "                return Tensor(result,grad='NA')\n",
    "    \n",
    "    def __neg__(self):\n",
    "        result=-self.array\n",
    "        if self.calc_grad:\n",
    "            grad={}\n",
    "            for w in self.grad:\n",
    "                grad[w]=-self.grad[w]\n",
    "            return Tensor(result,grad=grad)\n",
    "        else:\n",
    "            return Tensor(result,grad='NA')\n",
    "        \n",
    "    def sum(self,axis):\n",
    "        result=self.array.sum(axis=axis)\n",
    "        if self.calc_grad:\n",
    "            grad={}\n",
    "            for w in self.grad:\n",
    "                if self.grad[w] is not 0:\n",
    "                    grad[w]=self.grad[w].sum(axis=axis)\n",
    "                else:\n",
    "                    grad[w]=0\n",
    "            return Tensor(result,grad=grad)\n",
    "        else:\n",
    "            return Tensor(result,grad='NA')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Tensor({self.array},dtype {self.array.dtype},requires_grad={self.requires_grad})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    returns: Tensor with gradients\n",
    "    \"\"\"\n",
    "    def __call__(self,x):\n",
    "\n",
    "        u=np.exp(-x.array)\n",
    "        out=1/(1+u)\n",
    "\n",
    "        if Tensor.calc_grad:\n",
    "            grad={}\n",
    "            for w in x.grad:\n",
    "                if x.grad[w] is not 0:\n",
    "                    i=x.ndim\n",
    "                    l=x.grad[w].ndim\n",
    "                    expand=tuple([k for k in range(i,l)])\n",
    "                    grad_func=self.grad(u)\n",
    "                    grad_func=np.expand_dims(grad_func,axis=expand)\n",
    "                    grad[w]=grad_func*x.grad[w]\n",
    "                else:\n",
    "                    grad[w]=0\n",
    "\n",
    "            return Tensor(out,grad=grad)\n",
    "        else:\n",
    "            return Tensor(out,grad='NA')\n",
    "\n",
    "    @staticmethod\n",
    "    def grad(u):\n",
    "        den=(1+u)*(1+u)\n",
    "        gd=u/den\n",
    "\n",
    "        return gd\n",
    "\n",
    "class Log:\n",
    "\n",
    "    def __call__(self,x):\n",
    "        out=np.log(x.array)\n",
    "\n",
    "        grad={}\n",
    "        for w in x.grad:\n",
    "            if x.grad[w] is not 0:\n",
    "                i=x.ndim\n",
    "                l=x.grad[w].ndim\n",
    "                expand=tuple([k for k in range(i,l)])\n",
    "                grad_func=self.grad(x)\n",
    "                grad_func=np.expand_dims(grad_func,axis=expand)\n",
    "                grad[w]=grad_func*x.grad[w]\n",
    "            else:\n",
    "                grad[w]=0\n",
    "\n",
    "        return Tensor(out,grad=grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def grad(x):\n",
    "        gd=1/x.array\n",
    "\n",
    "        return gd\n",
    "\n",
    "class ReLU:\n",
    "    def __call__(self,x):\n",
    "        sign=(x.array<0)\n",
    "        z=x.array.copy()\n",
    "        z[sign]=0\n",
    "\n",
    "        if Tensor.calc_grad:\n",
    "            grad={}\n",
    "            for w in x.grad:\n",
    "                if x.grad[w] is not 0:\n",
    "                    i=x.ndim\n",
    "                    l=x.grad[w].ndim\n",
    "                    expand=tuple([k for k in range(i,l)])\n",
    "                    grad_func=self.grad(x,sign)\n",
    "                    grad_func=np.expand_dims(grad_func,axis=expand)\n",
    "                    grad[w]=grad_func*x.grad[w]\n",
    "                else:\n",
    "                    grad[w]=0\n",
    "\n",
    "            return Tensor(z,grad=grad)\n",
    "        else:\n",
    "            return Tensor(z,grad='NA')\n",
    "\n",
    "    @staticmethod\n",
    "    def grad(x,sign):\n",
    "        z=x.array.copy()\n",
    "        z[sign]=0\n",
    "        z[~sign]=1\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self,in_dim,out_dim,bias=True):\n",
    "        self.in_dim=in_dim\n",
    "        self.out_dim=out_dim\n",
    "\n",
    "        weight_,bias_=self.init_params()\n",
    "\n",
    "        self.weight=Tensor(weight_,requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias=Tensor(bias_,requires_grad=True)\n",
    "        else:\n",
    "            self.bias=0\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        \"\"\"\n",
    "        x: Tensor [batch,in_dim]\n",
    "        \"\"\"\n",
    "        out=x*self.weight+self.bias\n",
    "        return out\n",
    "\n",
    "    def init_params(self):\n",
    "        weight=np.random.normal(0,1,(self.in_dim,self.out_dim))\n",
    "        bias=np.random.normal(0,1,(1,self.out_dim))\n",
    "        return weight, bias\n",
    "\n",
    "class FeedForward:\n",
    "\n",
    "    def __init__(self,input_dim,hidden_dim,out_dim=1):\n",
    "        self.train() \n",
    "        self.in_layer=LinearLayer(input_dim,hidden_dim)\n",
    "        self.hid_layer=LinearLayer(hidden_dim,hidden_dim)\n",
    "        self.out_layer=LinearLayer(hidden_dim,out_dim)\n",
    "        self.relu=ReLU()\n",
    "        self.sig=Sigmoid()\n",
    "\n",
    "    def __call__(self,x):\n",
    "        \"\"\"\n",
    "        assume two class problem\n",
    "        \"\"\"\n",
    "        out=self.in_layer(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.hid_layer(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.out_layer(out)\n",
    "        out=self.sig(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        predict\n",
    "        \"\"\"\n",
    "        pred=self(x)\n",
    "        pred=pred.array.squeeze(1)\n",
    "        y_pred=(pred.array>=0.5).astype('int8')\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train(self):\n",
    "        Tensor.calc_grad=True\n",
    "    \n",
    "    def eval(self):\n",
    "        Tensor.calc_grad=False \n",
    "\n",
    "\n",
    "class LogLoss:\n",
    "    def __init__(self,model):\n",
    "        self.model=model\n",
    "        self.back_grads=None\n",
    "        self.log=Log()\n",
    "\n",
    "    def __call__(self,prob,y):\n",
    "        \n",
    "        not_y=(1-y.array).reshape(-1,1).T\n",
    "        not_y=Tensor(not_y)\n",
    "        y_=y.array.reshape(-1,1).T\n",
    "        y_=Tensor(y_)\n",
    "\n",
    "        not_prob=1-prob.array\n",
    "        grad={}\n",
    "        for w in prob.grad:\n",
    "            grad[w]=-prob.grad[w]\n",
    "        not_prob=Tensor(not_prob,grad=grad)\n",
    "\n",
    "        size=1/prob.shape[0]\n",
    "        L=y_*self.log(prob)+not_y*self.log(not_prob)\n",
    "        L=-L.sum(axis=0)\n",
    "        L=size*L\n",
    "\n",
    "        self.back_grads=L.grad\n",
    "\n",
    "        return L.array[0]\n",
    "    \n",
    "    def backward(self):\n",
    "        self.model.grads=self.back_grads\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self,model,lr=0.01):\n",
    "        self.model=model\n",
    "        self.lr=lr\n",
    "        self.tensors=self.find_tensor()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for idx, tensor in self.tensors.items():\n",
    "            if tensor.requires_grad:\n",
    "                grad={}\n",
    "                grad[idx]=tensor.grad[idx]\n",
    "                tensor.grad=grad\n",
    "            else:\n",
    "                grad={'none':0}\n",
    "                tensor.grad=grad \n",
    "\n",
    "    def step(self):\n",
    "        if self.model.grads is not None:\n",
    "            for idx, tensor in self.tensors.items():\n",
    "                if idx in self.model.grads:\n",
    "                    tensor.array-=self.lr*self.model.grads[idx].squeeze(0)\n",
    "        else:\n",
    "            print('No grads!')\n",
    "\n",
    "    def find_tensor(self):\n",
    "        tensors={}\n",
    "        for _,param1 in self.model.__dict__.items():\n",
    "            if isinstance(param1,Tensor):\n",
    "                tensors[id(param1)]=param1\n",
    "            elif hasattr(param1,'__dict__'):\n",
    "                for _,param2 in param1.__dict__.items():\n",
    "                    if isinstance(param2,Tensor):\n",
    "                        tensors[id(param2)]=param2\n",
    "        return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self,x,y,batch_size=28):\n",
    "        self.data_x=x\n",
    "        self.data_y=y\n",
    "        self.bsz=batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return x.shape[0]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        L=self.data_x.shape[0]\n",
    "        bsz=self.bsz\n",
    "        for i in range(0,L,bsz):\n",
    "            batch_x=Tensor(self.data_x[i:i+bsz])\n",
    "            batch_y=Tensor(self.data_y[i:i+bsz])\n",
    "            yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_breast_cancer()\n",
    "x=data['data']\n",
    "y=data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x/x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader=DataSet(x,y,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FeedForward(30,50)\n",
    "loss=LogLoss(model)\n",
    "opt=Optimizer(model,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,loss,optimizer,data_loader,epochs):\n",
    "        \n",
    "    L=len(data_loader)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss=0\n",
    "        for batch in data_loader:\n",
    "            x_batch, y_batch=batch\n",
    "            bsz=x_batch.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out=model(x_batch)\n",
    "            total_loss+=loss(out,y_batch)*bsz\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if epoch%2=10==0:\n",
    "            print('Loss: ',total_loss/L)\n",
    "    \n",
    "train(model,loss,opt,data_loader,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model,data_loader):\n",
    "    acc=0\n",
    "    model.eval()\n",
    "    for batch in data_loader:\n",
    "        x_b,y_b=batch \n",
    "        out=model(x_b)\n",
    "        pred=(out.array>0.5).astype('int8')\n",
    "        acc+=(y_b.array==pred.squeeze(1)).sum()\n",
    "    \n",
    "    return acc/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9086115992970123"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}